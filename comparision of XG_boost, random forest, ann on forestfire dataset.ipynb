{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Algerian_forest_fires_dataset_2020.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not fire' 'not fire' 'not fire' 'not fire' 'not fire' 'fire' 'fire'\n",
      " 'fire' 'not fire' 'not fire' 'fire' 'fire' 'not fire' 'not fire'\n",
      " 'not fire' 'not fire' 'not fire' 'not fire' 'not fire' 'not fire' 'fire'\n",
      " 'not fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'not fire' 'fire'\n",
      " 'not fire' 'not fire' 'not fire' 'not fire' 'fire' 'fire' 'not fire'\n",
      " 'fire' 'not fire' 'not fire' 'not fire' 'not fire' 'not fire' 'not fire'\n",
      " 'not fire' 'not fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'not fire'\n",
      " 'not fire' 'not fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire'\n",
      " 'not fire' 'not fire' 'not fire' 'fire' 'fire' 'fire' 'fire' 'not fire'\n",
      " 'fire' 'fire' 'fire' 'not fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'fire' 'fire   ' 'fire' 'fire ' 'fire' 'fire ' 'fire   ' 'not fire   '\n",
      " 'not fire' 'not fire ' 'not fire' 'not fire   ' 'not fire   ' 'fire   '\n",
      " 'not fire   ' 'not fire   ' 'not fire   ' 'not fire   ' 'not fire   '\n",
      " 'not fire   ' 'not fire   ' 'not fire   ' 'not fire   ' 'not fire   '\n",
      " 'not fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire' 'fire   '\n",
      " 'not fire   ' 'not fire   ' 'not fire   ' 'not fire   ' 'not fire   '\n",
      " 'fire   ' 'not fire   ' 'not fire   ' 'not fire   ' 'not fire   '\n",
      " 'not fire   ' 'not fire   ' 'not fire   ' 'not fire   ' 'fire   '\n",
      " 'fire   ' 'not fire   ' 'not fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'not fire   ' 'not fire   ' 'not fire   ' 'not fire   ' 'not fire   '\n",
      " 'not fire   ' 'not fire   ' 'not fire   ' 'fire   ' 'not fire   '\n",
      " 'not fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'not fire     ' 'not fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'not fire   ' 'not fire   '\n",
      " 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'not fire   '\n",
      " 'not fire   ' 'not fire   ' 'not fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'fire   ' 'not fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'not fire   ' 'not fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'fire   ' 'fire   ' 'not fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'not fire   ' 'not fire   ' 'fire   ' 'not fire   ' 'not fire   '\n",
      " 'not fire   ' 'fire   ' 'fire   ' 'fire   ' 'not fire   ' 'not fire   '\n",
      " 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   ' 'fire   '\n",
      " 'fire   ' 'not fire   ' 'fire   ' 'fire   ' 'fire   ' 'not fire   '\n",
      " 'not fire   ' 'fire   ' 'not fire   ' 'not fire   ' 'not fire   '\n",
      " 'not fire    ']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove white spaces in dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244,)\n",
      "['not fire' 'not fire' 'not fire' 'not fire' 'not fire' 'fire' 'fire'\n",
      " 'fire' 'not fire' 'not fire' 'fire' 'fire' 'not fire' 'not fire'\n",
      " 'not fire' 'not fire' 'not fire' 'not fire' 'not fire' 'not fire' 'fire'\n",
      " 'not fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'not fire' 'fire'\n",
      " 'not fire' 'not fire' 'not fire' 'not fire' 'fire' 'fire' 'not fire'\n",
      " 'fire' 'not fire' 'not fire' 'not fire' 'not fire' 'not fire' 'not fire'\n",
      " 'not fire' 'not fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'not fire'\n",
      " 'not fire' 'not fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire'\n",
      " 'not fire' 'not fire' 'not fire' 'fire' 'fire' 'fire' 'fire' 'not fire'\n",
      " 'fire' 'fire' 'fire' 'not fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire'\n",
      " 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire'\n",
      " 'fire' 'not fire' 'not fire' 'not fire' 'not fire' 'not fire' 'not fire'\n",
      " 'fire' 'not fire' 'not fire' 'not fire' 'not fire' 'not fire' 'not fire'\n",
      " 'not fire' 'not fire' 'not fire' 'not fire' 'not fire' 'fire' 'fire'\n",
      " 'fire' 'fire' 'fire' 'not fire' 'not fire' 'not fire' 'not fire'\n",
      " 'not fire' 'fire' 'not fire' 'not fire' 'not fire' 'not fire' 'not fire'\n",
      " 'not fire' 'not fire' 'not fire' 'fire' 'fire' 'not fire' 'not fire'\n",
      " 'fire' 'fire' 'fire' 'not fire' 'not fire' 'not fire' 'not fire'\n",
      " 'not fire' 'not fire' 'not fire' 'not fire' 'fire' 'not fire' 'not fire'\n",
      " 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'not fire' 'not fire' 'fire'\n",
      " 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'not fire' 'not fire'\n",
      " 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire'\n",
      " 'fire' 'fire' 'not fire' 'not fire' 'not fire' 'not fire' 'fire' 'fire'\n",
      " 'fire' 'fire' 'not fire' 'fire' 'fire' 'fire' 'fire' 'not fire'\n",
      " 'not fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire'\n",
      " 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire'\n",
      " 'fire' 'not fire' 'fire' 'fire' 'fire' 'not fire' 'not fire' 'fire'\n",
      " 'not fire' 'not fire' 'not fire' 'fire' 'fire' 'fire' 'not fire'\n",
      " 'not fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire' 'fire'\n",
      " 'not fire' 'fire' 'fire' 'fire' 'not fire' 'not fire' 'fire' 'not fire'\n",
      " 'not fire' 'not fire' 'not fire']\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "yd=list()\n",
    "for i in y:\n",
    "    i=i.rstrip()\n",
    "    yd.append(i)\n",
    "yd=np.array(yd)\n",
    "print(yd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Dependent Variable (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "yd = le.fit_transform(yd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1\n",
      " 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
      " 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(yd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yd, test_size = 0.12, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:06:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier #for regression: XGBRegressor\n",
    "classifier = XGBClassifier()  #no need to tune parameter\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy for XG_Boost using k-fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohan/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:06:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:06:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:06:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:06:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:06:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:06:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:06:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:06:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:06:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:06:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 96.30 %\n",
      "Standard Deviation: 4.03 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Random Forest for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "rclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy for Random Forest using k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.21 %\n",
      "Standard Deviation: 3.12 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = rclassifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding input and 1st Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding 2nd Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0687 - accuracy: 0.9673\n",
      "Epoch 2/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0681 - accuracy: 0.9673\n",
      "Epoch 3/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0675 - accuracy: 0.9673\n",
      "Epoch 4/45\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.9673\n",
      "Epoch 5/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0663 - accuracy: 0.9673\n",
      "Epoch 6/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9673\n",
      "Epoch 7/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0651 - accuracy: 0.9720\n",
      "Epoch 8/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.9720\n",
      "Epoch 9/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0640 - accuracy: 0.9673\n",
      "Epoch 10/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9673\n",
      "Epoch 11/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0630 - accuracy: 0.9720\n",
      "Epoch 12/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0624 - accuracy: 0.9673\n",
      "Epoch 13/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.9673\n",
      "Epoch 14/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0614 - accuracy: 0.9720\n",
      "Epoch 15/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 0.9720\n",
      "Epoch 16/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0604 - accuracy: 0.9720\n",
      "Epoch 17/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0598 - accuracy: 0.9720\n",
      "Epoch 18/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0594 - accuracy: 0.9720\n",
      "Epoch 19/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0589 - accuracy: 0.9720\n",
      "Epoch 20/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0583 - accuracy: 0.9720\n",
      "Epoch 21/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0578 - accuracy: 0.9720\n",
      "Epoch 22/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9766\n",
      "Epoch 23/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0570 - accuracy: 0.9720\n",
      "Epoch 24/45\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0563 - accuracy: 0.9766\n",
      "Epoch 25/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9766\n",
      "Epoch 26/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9766\n",
      "Epoch 27/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0551 - accuracy: 0.9766\n",
      "Epoch 28/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0545 - accuracy: 0.9766\n",
      "Epoch 29/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0540 - accuracy: 0.9766\n",
      "Epoch 30/45\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0536 - accuracy: 0.9766\n",
      "Epoch 31/45\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0533 - accuracy: 0.9766\n",
      "Epoch 32/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9766\n",
      "Epoch 33/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0523 - accuracy: 0.9766\n",
      "Epoch 34/45\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0521 - accuracy: 0.9766\n",
      "Epoch 35/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0515 - accuracy: 0.9766\n",
      "Epoch 36/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0512 - accuracy: 0.9766\n",
      "Epoch 37/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0507 - accuracy: 0.9766\n",
      "Epoch 38/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9766\n",
      "Epoch 39/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9766\n",
      "Epoch 40/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0496 - accuracy: 0.9766\n",
      "Epoch 41/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 0.9766\n",
      "Epoch 42/45\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0487 - accuracy: 0.9813\n",
      "Epoch 43/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0485 - accuracy: 0.9813\n",
      "Epoch 44/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0480 - accuracy: 0.9813\n",
      "Epoch 45/45\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0475 - accuracy: 0.9813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f27bc7a3160>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of ANN using confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "#print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
